<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Courier New:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="DL,TensorFlow," />





  <link rel="alternate" href="/atom.xml" title="飘" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="ReferrenceTensorflow常用函数说明Tensorflow入门教程合集图解TensorFlowTensorFlow博客资源Tensorflow学习资料">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow常用函数">
<meta property="og:url" content="http://yoursite.com/2017/03/20/Tensorflow常用函数/index.html">
<meta property="og:site_name" content="飘">
<meta property="og:description" content="ReferrenceTensorflow常用函数说明Tensorflow入门教程合集图解TensorFlowTensorFlow博客资源Tensorflow学习资料">
<meta property="og:updated_time" content="2017-06-19T13:33:47.572Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tensorflow常用函数">
<meta name="twitter:description" content="ReferrenceTensorflow常用函数说明Tensorflow入门教程合集图解TensorFlowTensorFlow博客资源Tensorflow学习资料">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/2017/03/20/Tensorflow常用函数/"/>


<!-- 网页加载条 -->
<script src="/js/src/pace.min.js"></script>
  <title> Tensorflow常用函数 | 飘 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">飘</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">海の上で最も自由なのは海賊王だぁ～</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      

      <!-- 自定义High一下的功能 -->
      <li class="menu-item"> <a title="把这个链接拖到你的工具栏中,任何网页都可以High" href='javascript:(
    /*
     * Copyright (C) 2016 Never_yu (Neveryu.github.io) <React.dong.yu@gmail.com>
     * Sina Weibo (http://weibo.com/1134446353)
     *
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     *      http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     */
    function go() {


    var songs = [
                "http://bdyun.65dj.com:8090/2015/06/28/84791C997D8C55023DAD0D5690E48C28/%D1%A6%D6%AE%C7%AB%20-%20%D1%DD%D4%B1.mp3",
                "http://7xoiki.com1.z0.glb.clouddn.com/Music-sunburst.mp3",
                ""
    ];

    
    function c() {
        var e = document.createElement("link");
        e.setAttribute("type", "text/css");
        e.setAttribute("rel", "stylesheet");
        e.setAttribute("href", f);
        e.setAttribute("class", l);
        document.body.appendChild(e)
    }
 
    function h() {
        var e = document.getElementsByClassName(l);
        for (var t = 0; t < e.length; t++) {
            document.body.removeChild(e[t])
        }
    }
 
    function p() {
        var e = document.createElement("div");
        e.setAttribute("class", a);
        document.body.appendChild(e);
        setTimeout(function() {
            document.body.removeChild(e)
        }, 100)
    }
 
    function d(e) {
        return {
            height : e.offsetHeight,
            width : e.offsetWidth
        }
    }
 
    function v(i) {
        var s = d(i);
        return s.height > e && s.height < n && s.width > t && s.width < r
    }
 
    function m(e) {
        var t = e;
        var n = 0;
        while (!!t) {
            n += t.offsetTop;
            t = t.offsetParent
        }
        return n
    }
 
    function g() {
        var e = document.documentElement;
        if (!!window.innerWidth) {
            return window.innerHeight
        } else if (e && !isNaN(e.clientHeight)) {
            return e.clientHeight
        }
        return 0
    }
 
    function y() {
        if (window.pageYOffset) {
            return window.pageYOffset
        }
        return Math.max(document.documentElement.scrollTop, document.body.scrollTop)
    }
 
    function E(e) {
        var t = m(e);
        return t >= w && t <= b + w
    }

    function S() {
        var e = document.getElementById("audio_element_id");
        if(e != null){
            var index = parseInt(e.getAttribute("curSongIndex"));
            if(index > songs.length - 2) {
                index = 0;
            } else {
                index++;
            }
            e.setAttribute("curSongIndex", index);
            N();
        }

        e.src = i;
        e.play()
    }
 
    function x(e) {
        e.className += " " + s + " " + o
    }
 
    function T(e) {
        e.className += " " + s + " " + u[Math.floor(Math.random() * u.length)]
    }
 
    function N() {
        var e = document.getElementsByClassName(s);
        var t = new RegExp("\\b" + s + "\\b");
        for (var n = 0; n < e.length; ) {
            e[n].className = e[n].className.replace(t, "")
        }
    }

    function initAudioEle() {
        var e = document.getElementById("audio_element_id");
        if(e === null){
            e = document.createElement("audio");
            e.setAttribute("class", l);
            e.setAttribute("curSongIndex", 0);
            e.id = "audio_element_id";
            e.loop = false;
            e.bgcolor = 0;
            e.addEventListener("canplay", function() {
            setTimeout(function() {
                x(k)
            }, 500);
            setTimeout(function() {
                N();
                p();
                for (var e = 0; e < O.length; e++) {
                    T(O[e])
                }
            }, 15500)
        }, true);
        e.addEventListener("ended", function() {
            N();
            h();
            go();
        }, true);
        e.innerHTML = " <p>If you are reading this, it is because your browser does not support the audio element. We recommend that you get a new browser.</p> <p>";
        document.body.appendChild(e);
        }
    }
    
    initAudioEle();
    var e = 30;
    var t = 30;
    var n = 350;
    var r = 350;

    var curSongIndex = parseInt(document.getElementById("audio_element_id").getAttribute("curSongIndex"));
    var i = songs[curSongIndex];
    
    var s = "mw-harlem_shake_me";
    var o = "im_first";
    var u = ["im_drunk", "im_baked", "im_trippin", "im_blown"];
    var a = "mw-strobe_light";

    /* harlem-shake-style.css，替换成你的位置，也可以直接使用：//s3.amazonaws.com/moovweb-marketing/playground/harlem-shake-style.css */
    var f = "//s3.amazonaws.com/moovweb-marketing/playground/harlem-shake-style.css";
    
    var l = "mw_added_css";
    var b = g();
    var w = y();
    var C = document.getElementsByTagName("*");
    var k = null;
    for (var L = 0; L < C.length; L++) {
        var A = C[L];
        if (v(A)) {
            if (E(A)) {
                k = A;
                break
            }
        }
    }
    if (A === null) {
        console.warn("Could not find a node of the right size. Please try a different page.");
        return
    }
    c();
    S();
    var O = [];
    for (var L = 0; L < C.length; L++) {
        var A = C[L];
        if (v(A)) {
            O.push(A)
        }
    }
    })()'><i class="menu-item-icon fa fa-music fa-fw"></i>High一下</a> </li>
      <!-- end High一下 -->
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search fa-lg"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Tensorflow常用函数
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-20T17:34:58+08:00" content="2017-03-20">
              2017-03-20
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/DL/" itemprop="url" rel="index">
                    <span itemprop="name">DL</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Referrence"><a href="#Referrence" class="headerlink" title="Referrence"></a>Referrence</h1><p><a href="http://www.cnblogs.com/wuzhitj/p/6298004.html" target="_blank" rel="external">Tensorflow常用函数说明</a><br><a href="http://blog.csdn.net/jdbc/article/details/52402302" target="_blank" rel="external">Tensorflow入门教程合集</a><br><a href="http://www.cnblogs.com/yao62995/p/5773578.html" target="_blank" rel="external">图解TensorFlow</a><br><a href="http://blog.csdn.net/mydear_11000/article/details/52879710" target="_blank" rel="external">TensorFlow博客资源</a><br><a href="http://www.snakehacker.me/403" target="_blank" rel="external">Tensorflow学习资料</a></p>
<a id="more"></a> 
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://blog.csdn.net/jdbc/article/details/68957085" target="_blank" rel="external">TensorFlow入门教程之0: BigPicture&amp;极速入门</a><br><a href="https://my.oschina.net/yilian/blog/659618" target="_blank" rel="external">TensorFlow入门教程之1: 基本概念以及理解</a><br><a href="http://blog.csdn.net/kkk584520/article/details/51476816" target="_blank" rel="external">TensorFlow入门教程之2: 安装和使用</a><br><a href="http://my.oschina.net/yilian/blog/661218" target="_blank" rel="external">TensorFlow入门教程之3: CNN卷积神经网络的基本定义理解</a><br><a href="https://my.oschina.net/yilian/blog/661409" target="_blank" rel="external">TensorFlow入门教程之4: 实现一个自创的CNN卷积神经网络</a><br><a href="http://my.oschina.net/yilian/blog/661900" target="_blank" rel="external">TensorFlow入门教程之5: TensorBoard面板可视化管理</a><br><a href="http://my.oschina.net/yilian/blog/662029" target="_blank" rel="external">TensorFlow入门教程之6: AlphaGo 的策略网络（CNN）简单的实现</a><br><a href="http://my.oschina.net/yilian/blog/662539" target="_blank" rel="external">TensorFlow入门教程之7: 训练的模型Model 保存 文件 并使用</a><br><a href="http://my.oschina.net/yilian/blog/664077" target="_blank" rel="external">TensorFlow入门教程之8: DNN深度神经网络 的原理 以及 使用</a><br><a href="http://my.oschina.net/yilian/blog/664087" target="_blank" rel="external">TensorFlow入门教程之9: 接着补充一章MLP多层感知器网络原理以及 使用</a><br><a href="http://my.oschina.net/yilian/blog/665412" target="_blank" rel="external">TensorFlow入门教程之10: RNN循环网络原理以及使用</a><br><a href="http://blog.csdn.net/kkk584520/article/details/51477830" target="_blank" rel="external">TensorFlow入门教程之11: 使用TensorFlow实现RNN</a><br><a href="http://my.oschina.net/yilian/blog/667900" target="_blank" rel="external">TensorFlow入门教程之12: 最强网络RSNN深度残差网络 平均准确率96-99%</a><br><a href="http://my.oschina.net/yilian/blog/667904" target="_blank" rel="external">TensorFlow入门教程之13: 最强网络DLSTM 双向长短期记忆网络（阿里小AI实现）</a><br><a href="http://my.oschina.net/yilian/blog/672135" target="_blank" rel="external">TensorFlow入门教程之14: Tensorflow Caffe相互转换</a><br><a href="http://my.oschina.net/yilian/blog/673051" target="_blank" rel="external">TensorFlow入门教程之15: Tensorflow RCNN区域卷积神经网络</a><br><a href="http://my.oschina.net/yilian/blog/687364" target="_blank" rel="external">Tensorflow入门教程之16: 模型AutoEncoder自编码机网络</a><br><a href="http://my.oschina.net/yilian/blog/693660" target="_blank" rel="external">Tensorflow入门教程之17: Tensorflow人工智能分布式实现</a><br>查看TensorFlow版本：<br>    import tensorflow as tf<br>    tf.<strong>version</strong><br>    tf.<strong>path</strong></p>
<p>查看Linux版本<br>    cat /etc/issue<br>    cat /proc/version</p>
<h1 id="tensorflow自学之前的bigpicture"><a href="#tensorflow自学之前的bigpicture" class="headerlink" title="tensorflow自学之前的bigpicture"></a>tensorflow自学之前的bigpicture</h1><p>1、tf.contrib.learn，tf.contrib.slim，Keras 等，它们都提供了高层封装<br>2、计算图：有向无环图<br>3、相关概念<br>    Tensor：类型化的多维数组，图的边；<br>    Operation:执行计算的单元，图的节点；<br>    Graph：一张有边与点的图，其表示了需要进行计算的任务；<br>    Session:称之为会话的上下文，用于执行图。<br>Graph仅仅定义了所有 operation 与 tensor 流向，没有进行任何计算。而session根据 graph 的定义分配资源，计算 operation，得出结果。既然是图就会有点与边，在图计算中 operation 就是点而 tensor 就是边。Operation 可以是加减乘除等数学运算，也可以是各种各样的优化算法。每个 operation 都会有零个或多个输入，零个或多个输出。 tensor 就是其输入与输出，其可以表示一维二维多维向量或者常量。而且除了Variables指向的 tensor 外所有的 tensor 在流入下一个节点后都不再保存。<br>4、数据结构<br>    rank：Rank一般是指数据的维度，其与线性代数中的rank不是一个概念<br>    Shape：Shape指tensor每个维度数据的个数，可以用python的list/tuple表示。<br>    data type：Data type，是指单个数据的类型。常用DT_FLOAT，也就是32位的浮点数。<br>5、Variables<br>5.1、介绍<br>当训练模型时，需要使用Variables保存与更新参数。Variables会保存在内存当中，所有tensor一旦拥有Variables的指向就不会在session中丢失。其必须明确的初始化而且可以通过Saver保存到磁盘上。Variables可以通过Variables初始化。<br>5.2初始化<br>实际在其初始化过程中做了很多的操作，比如初始化空间，赋初值（等价于tf.assign），并把Variable添加到graph中等操作。<br>5.3Variables与constant的区别<br>Constant一般是常量，可以被赋值给Variables，constant保存在graph中，如果graph重复载入那么constant也会重复载入，其非常浪费资源，如非必要尽量不使用其保存大量数据。<br>5.4命名<br>另外一个值得注意的地方是尽量每一个变量都明确的命名，这样易于管理命令空间，而且在导入模型的时候不会造成不同模型之间的命名冲突，这样就可以在一张graph中容纳很多个模型。<br>6、placeholders与feed_dict<br>当我们定义一张graph时，有时候并不知道需要计算的值，比如模型的输入数据，其只有在训练与预测时才会有值。这时就需要placeholder与feed_dict的帮助。</p>
<h1 id="基本概念以及理解"><a href="#基本概念以及理解" class="headerlink" title="基本概念以及理解"></a>基本概念以及理解</h1><p><a href="https://my.oschina.net/yilian/blog/659618" target="_blank" rel="external">https://my.oschina.net/yilian/blog/659618</a></p>
<h1 id="安装和使用"><a href="#安装和使用" class="headerlink" title="安装和使用"></a>安装和使用</h1><p><a href="http://blog.csdn.net/kkk584520/article/details/51476816" target="_blank" rel="external">http://blog.csdn.net/kkk584520/article/details/51476816</a></p>
<h1 id="CNN卷积神经网络的基本定义理解"><a href="#CNN卷积神经网络的基本定义理解" class="headerlink" title="CNN卷积神经网络的基本定义理解"></a>CNN卷积神经网络的基本定义理解</h1><pre><code>def conv2d(x, w, b):
    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;),b)) #卷积层 定义 relu(wx+b)  
x = tf.placeholder(tf.float32, [None, w*h]) 
y = tf.placeholder(tf.float32, [None, ysize])#y的数目个数 比如3个类 就是3
wc=tf.Variable(tf.random_normal([3, 3, 1, 64]) #3 3 分别为3x3大小的卷积核 1为输入数目 
#因为是第一层所以是1 输出我们配置的64 ，初步理解这个输出是卷积核的个数
#所以我们知道了 如果下一次卷积wc2=[5,5,64,256] 
#5x5 是我们配置的卷积核大小，第三位表示输入数目 我们通过上面知道 上面的输出 也就是下一层的输入 所以 就是64 #了输出我们定义成256 #这个随你喜好，关键要迭代看效果，一般都是某一个v*v的值
b1=tf.Variable(tf.random_normal([64]))    #同样可以知道b2=[256]
def max_pool_kxk(x):
    return tf.nn.max_pool(x, ksize=[1, k, k, 1],
                    strides=[1, k, k, 1], padding=&apos;SAME&apos;)# 参数的含义需要进一步考证
#池化层k*k里面取最大值
</code></pre><p>eg：</p>
<pre><code>import input_data
mnist = input_data.read_data_sets(&quot;/tmp/data/&quot;, one_hot=True)

import tensorflow as tf

# Parameters
learning_rate = 0.001
training_iters = 100000
batch_size = 128
display_step = 10

# Network Parameters
n_input = 784 # MNIST data input (img shape: 28*28)
n_classes = 10 # MNIST total classes (0-9 digits)
dropout = 0.75 # Dropout, probability to keep units

# tf Graph input
x = tf.placeholder(tf.float32, [None, n_input])
y = tf.placeholder(tf.float32, [None, n_classes])
keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)

# Create model
def conv2d(img, w, b):
    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;),b))

    #tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)
    #除去name参数用以指定该操作的name，与方法有关的一共五个参数：
    #第一个参数input：指需要做卷积的输入图像，它要求是一个Tensor，具有[batch,in_height,in_width,in_channels]这样的shape，具体含义是[训练时一个batch的图片
    #数量，图片高度, 图片宽度,     图像通道数]，注意这是一个4维的Tensor，要求类型为float32和float64其中之一
    #第二个参数filter：相当于CNN中的卷积核，它要求是一个Tensor，具有[filter_height,filter_width,in_channels,out_channels]这样的shape，具体含义是[卷积核的高度#，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数input相同，有一个地方需要注意，第三维in_channels#，就是参数input的第四维
    #第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4
    #第四个参数padding：string类型的量，只能是&quot;SAME&quot;,&quot;VALID&quot;其中之一，这个值决定了不同的卷积方式（后面会介绍）
    #&quot;Same&quot;表示表示卷积核可以停留在图像边缘，valid只考虑有效的像素点
    #第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true
    #结果返回一个Tensor，这个输出，就是我们常说的feature map
    #http://blog.csdn.net/mao_xiao_feng/article/details/53444333

def max_pool(img, k):
    return tf.nn.max_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=&apos;SAME&apos;)

    #tf.nn.max_pool(value, ksize, strides, padding, name=None)
    #参数是四个，和卷积很类似：
    #第一个参数value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape
    #第二个参数ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1
    #第三个参数strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]
    #第四个参数padding：和卷积类似，可以取&apos;VALID&apos; 或者&apos;SAME&apos;
    #返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式

def conv_net(_X, _weights, _biases, _dropout):
    # Reshape input picture
    _X = tf.reshape(_X, shape=[-1, 28, 28, 1]) # -1表示会自动计算出来该维的值，注意最后面的1

    # Convolution Layer
    conv1 = conv2d(_X, _weights[&apos;wc1&apos;], _biases[&apos;bc1&apos;])
    # Max Pooling (down-sampling)
    conv1 = max_pool(conv1, k=2)
    # Apply Dropout
    conv1 = tf.nn.dropout(conv1, _dropout) # 按照一定的比例dropout某些节点，防止过拟合 参考 http://www.cnblogs.com/tornadomeet/p/3258122.html

    # Convolution Layer
    conv2 = conv2d(conv1, _weights[&apos;wc2&apos;], _biases[&apos;bc2&apos;])
    # Max Pooling (down-sampling)
    conv2 = max_pool(conv2, k=2) 
    # Apply Dropout
    conv2 = tf.nn.dropout(conv2, _dropout)

    # Fully connected layer
    dense1 = tf.reshape(conv2, [-1, _weights[&apos;wd1&apos;].get_shape().as_list()[0]]) # Reshape conv2 output to fit dense layer input
    dense1 = tf.nn.relu(tf.add(tf.matmul(dense1, _weights[&apos;wd1&apos;]), _biases[&apos;bd1&apos;])) # Relu activation
    dense1 = tf.nn.dropout(dense1, _dropout) # Apply Dropout

    # Output, class prediction
    out = tf.add(tf.matmul(dense1, _weights[&apos;out&apos;]), _biases[&apos;out&apos;])
    return out

# Store layers weight &amp; bias
weights = {
    &apos;wc1&apos;: tf.Variable(tf.random_normal([5, 5, 1, 32])), # 5x5 conv, 1 input, 32 outputs
    &apos;wc2&apos;: tf.Variable(tf.random_normal([5, 5, 32, 64])), # 5x5 conv, 32 inputs, 64 outputs
    &apos;wd1&apos;: tf.Variable(tf.random_normal([7*7*64, 1024])), # fully connected, 7*7*64 inputs, 1024 outputs
    &apos;out&apos;: tf.Variable(tf.random_normal([1024, n_classes])) # 1024 inputs, 10 outputs (class prediction)
}

biases = {
    &apos;bc1&apos;: tf.Variable(tf.random_normal([32])),
    &apos;bc2&apos;: tf.Variable(tf.random_normal([64])),
    &apos;bd1&apos;: tf.Variable(tf.random_normal([1024])),
    &apos;out&apos;: tf.Variable(tf.random_normal([n_classes]))
}

# Construct model
pred = conv_net(x, weights, biases, keep_prob)

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

# Evaluate model
correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1)) 
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

#tf.argmax(input, axis=None, name=None, dimension=None)
#Returns the index with the largest value across axis of a tensor.
#tf.equal(a,b) 如果a,b是数组，返回长度一致的一维数组，每一项为1或者0，表示每一项是否相等 
#http://stackoverflow.com/questions/41708572/tensorflow-questions-regarding-tf-argmax-and-tf-equal

# Initializing the variables
init = tf.initialize_all_variables()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)
    step = 1
    # Keep training until reach max iterations
    while step * batch_size &lt; training_iters:
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        # Fit training using batch data
        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})
        if step % display_step == 0:
            # Calculate batch accuracy
            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})
            # Calculate batch loss
            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})
            print &quot;Iter &quot; + str(step*batch_size) + &quot;, Minibatch Loss= &quot; + &quot;{:.6f}&quot;.format(loss) + &quot;, Training Accuracy= &quot; + &quot;{:.5f}&quot;.format(acc)
        step += 1
    print &quot;Optimization Finished!&quot;
    # Calculate accuracy for 256 mnist test images
    print &quot;Testing Accuracy:&quot;, sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.})
</code></pre><p>常用函数：<br>    tf.concat(concat_dim, values, name=’concat’)<br>    tf.concat是连接两个矩阵的操作<br>    除去name参数用以指定该操作的name，与方法有关的一共两个参数：<br>    第一个参数concat_dim：必须是一个数，表明在哪一维上连接<br>    如果concat_dim是0，那么在某一个shape的第一个维度上连，对应到实际，就是叠放到列上<br>    <a href="http://blog.csdn.net/mao_xiao_feng/article/details/53366163" target="_blank" rel="external">http://blog.csdn.net/mao_xiao_feng/article/details/53366163</a></p>
<pre><code>tf.expand_dims(tf.constant([1,2,3]),1)  
扩维操作

tf.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value, name=None)
除去name参数用以指定该操作的name，与方法有关的一共四个参数：
第一个参数sparse_indices：稀疏矩阵中那些个别元素对应的索引值。
    有三种情况：
    sparse_indices是个数，那么它只能指定一维矩阵的某一个元素
    sparse_indices是个向量，那么它可以指定一维矩阵的多个元素
    sparse_indices是个矩阵，那么它可以指定二维矩阵的多个元素
第二个参数output_shape：输出的稀疏矩阵的shape
第三个参数sparse_values：个别元素的值。
    分为两种情况：
    sparse_values是个数：所有索引指定的位置都用这个数
    sparse_values是个向量：输出矩阵的某一行向量里某一行对应的数（所以这里向量的长度应该和输出矩阵的行数对应，不然报错）
第四个参数default_value：未指定元素的默认值，一般如果是稀疏矩阵的话就是0了
http://blog.csdn.net/mao_xiao_feng/article/details/53365889

tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)
除去name参数用以指定该操作的name，与方法有关的一共两个参数：
第一个参数logits：就是神经网络最后一层的输出，如果有batch的话，它的大小就是[batchsize，num_classes]，单样本的话，大小就是num_classes
第二个参数labels：实际的标签，大小同上
第一步：对网络最后一层的输出做一个softmax
第二步：softmax的输出向量[Y1，Y2,Y3...]和样本的实际标签做一个交叉熵
tf.reduce_sum操作,就是对向量里面所有元素求和
tf.reduce_mean操作，对向量求均值
http://blog.csdn.net/mao_xiao_feng/article/details/53382790

tf.nn.local_response_normalization(input, depth_radius=None, bias=None, alpha=None, beta=None, name=None) = tf.nn.lrn
http://blog.csdn.net/mao_xiao_feng/article/details/53488271
除去name参数用以指定该操作的name，与方法有关的一共五个参数：
第一个参数input：这个输入就是feature map了，既然是feature map，那么它就具有[batch, height, width, channels]这样的shape
第二个参数depth_radius：这个值需要自己指定，就是上述公式中的n/2
第三个参数bias：上述公式中的k
第四个参数alpha：上述公式中的α
第五个参数beta：上述公式中的β
返回值是新的feature map，它应该具有和原feature map相同的shape
</code></pre><h1 id="实现一个自创的CNN卷积神经网络"><a href="#实现一个自创的CNN卷积神经网络" class="headerlink" title="实现一个自创的CNN卷积神经网络"></a>实现一个自创的CNN卷积神经网络</h1><h1 id="TensorBoard面板可视化管理"><a href="#TensorBoard面板可视化管理" class="headerlink" title="TensorBoard面板可视化管理"></a>TensorBoard面板可视化管理</h1><p><a href="https://www.tensorflow.org/versions/r0.8/how_tos/summaries_and_tensorboard/index.html" target="_blank" rel="external">https://www.tensorflow.org/versions/r0.8/how_tos/summaries_and_tensorboard/index.html</a></p>
<h2 id="tensorflow-网络可视化"><a href="#tensorflow-网络可视化" class="headerlink" title="tensorflow 网络可视化"></a>tensorflow 网络可视化</h2><pre><code>summary_writer = tf.train.SummaryWriter(&apos;/tmp/tensorflowlogs&apos;, graph_def=sess.graph_def)
# 将来训练时候的op 过程记录到日志里面，便于后面显示graph
</code></pre><h2 id="4D数据虚拟化"><a href="#4D数据虚拟化" class="headerlink" title="4D数据虚拟化"></a>4D数据虚拟化</h2><pre><code>images = np.random.randint(256, size=shape).astype(np.uint8)
tf.image_summary(&quot;Visualize_image&quot;, images)#用图像方式虚拟化显示image_summary
</code></pre><h2 id="图标变化方式展示tensorflow数据特征"><a href="#图标变化方式展示tensorflow数据特征" class="headerlink" title="图标变化方式展示tensorflow数据特征"></a>图标变化方式展示tensorflow数据特征</h2><pre><code>w_hist = tf.histogram_summary(&quot;weights&quot;, W)
b_hist = tf.histogram_summary(&quot;biases&quot;, b)
y_hist = tf.histogram_summary(&quot;y&quot;, y)
</code></pre><h2 id="以变量批次事件变化的线路图显示表示tensorflow数据"><a href="#以变量批次事件变化的线路图显示表示tensorflow数据" class="headerlink" title="以变量批次事件变化的线路图显示表示tensorflow数据"></a>以变量批次事件变化的线路图显示表示tensorflow数据</h2><pre><code>accuracy_summary = tf.scalar_summary(&quot;accuracy&quot;, accuracy)
</code></pre><h2 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h2><pre><code>merged = tf.merge_all_summaries() #汇总summary
summary_all = sess.run(merged_summary_op, feed_dict={x: batch_xs, y: batch_ys})
summary_writer.add_summary(summary_all,  i) 
#session中执行汇总summary操作op 并使用上面sumarywriter 写入更新到graph 日志中
</code></pre><h1 id="AlphaGo-的策略网络（CNN）简单的实现"><a href="#AlphaGo-的策略网络（CNN）简单的实现" class="headerlink" title="AlphaGo 的策略网络（CNN）简单的实现"></a>AlphaGo 的策略网络（CNN）简单的实现</h1><p><a href="http://renzhichu1987.blogchina.com/2917056.html" target="_blank" rel="external">AlphaGo机制说明</a></p>
<h1 id="训练的模型Model保存文件并使用"><a href="#训练的模型Model保存文件并使用" class="headerlink" title="训练的模型Model保存文件并使用"></a>训练的模型Model保存文件并使用</h1><pre><code>save_path = saver.save(sess, &quot;/root/alexnet.tfmodel&quot;) #保存
saver.restore(sess, &quot;/root/alexnet.tfmodel&quot;) #使用
</code></pre><h1 id="DNN深度神经网络的原理以及使用"><a href="#DNN深度神经网络的原理以及使用" class="headerlink" title="DNN深度神经网络的原理以及使用"></a>DNN深度神经网络的原理以及使用</h1><p>DNN ,就是去掉卷积层之后  使用全连接层+dropout下降+relu激活  一层一层的WX+B的 网络模式 </p>
<h1 id="接着补充一章MLP多层感知器网络原理以及使用"><a href="#接着补充一章MLP多层感知器网络原理以及使用" class="headerlink" title="接着补充一章MLP多层感知器网络原理以及使用"></a>接着补充一章MLP多层感知器网络原理以及使用</h1><pre><code>linear----线性感知器

tanh----双曲正切函数

sigmoid----双曲函数

softmax----1/(e(net) * e(wi*xi- shift))

log-softmax---- log(1/(e(net) * e(wi*xi)))

exp----指数函数

softplus----log(1+ e(wi*xi))
</code></pre><h1 id="RNN循环网络原理以及使用"><a href="#RNN循环网络原理以及使用" class="headerlink" title="RNN循环网络原理以及使用"></a>RNN循环网络原理以及使用</h1><pre><code>def rnn(cell, inputs, initial_state, dtype=None, squence_length=None,scope=None)
参数：
cell：RNNCell
inputs:A length T list of inputs,each a vector with shape [batch_size]
initial_state:An initial state for RNN shape[batch_size*cell.state_size]
dtype:required if initial_state is not provided
sequence_length:An int64 vector size[batch_size]
scope:VariableScope for the created subgraph;default to &quot;RNN&quot;
returns :
A pair (outputs,state)where:
    output is a length T list of outputs(one for each input)
    states is a length T list of states(one state following each input)
_X = tf.transpose(_X, [1, 0, 2]) # 对于3维的X 默认是 0 1 2 系数 ，我们把它置换成1 0 2 也就是 第一维 第二维进行交换
</code></pre><p>eg：<br>    import input_data<br>    mnist = input_data.read_data_sets(“/tmp/data/“, one_hot=True)</p>
<pre><code>import tensorflow as tf
# from tensorflow.models.rnn import rnn, rnn_cell
from tensorflow.contrib import rnn
import numpy as np

&apos;&apos;&apos;
To classify images using a reccurent neural network, we consider every image row as a sequence of pixels.
Because MNIST image shape is 28*28px, we will then handle 28 sequences of 28 steps for every sample.
&apos;&apos;&apos;

# Parameters
learning_rate = 0.001
training_iters = 100000
batch_size = 128
display_step = 10

# Network Parameters
n_input = 28 # MNIST data input (img shape: 28*28)
n_steps = 28 # timesteps
n_hidden = 128 # hidden layer num of features
n_classes = 10 # MNIST total classes (0-9 digits)

# tf Graph input
x = tf.placeholder(&quot;float&quot;, [None, n_steps, n_input])
# Tensorflow LSTM cell requires 2x n_hidden length (state &amp; cell)
istate = tf.placeholder(&quot;float&quot;, [None, 2*n_hidden])
y = tf.placeholder(&quot;float&quot;, [None, n_classes])

# Define weights
weights = {
    &apos;hidden&apos;: tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights
    &apos;out&apos;: tf.Variable(tf.random_normal([n_hidden, n_classes]))
}
biases = {
    &apos;hidden&apos;: tf.Variable(tf.random_normal([n_hidden])),
    &apos;out&apos;: tf.Variable(tf.random_normal([n_classes]))
}

def RNN(_X, _istate, _weights, _biases):


    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size

    _X = tf.reshape(_X, [-1, n_input]) # (n_steps*batch_size, n_input)

    _X = tf.matmul(_X, _weights[&apos;hidden&apos;]) + _biases[&apos;hidden&apos;]


    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)

    _X = tf.split(0, n_steps, _X) # n_steps * (batch_size, n_hidden)


    outputs, states = rnn.rnn(lstm_cell, _X, initial_state=_istate)

    # Get inner loop last output
    return tf.matmul(outputs[-1], _weights[&apos;out&apos;]) + _biases[&apos;out&apos;]

pred = RNN(x, istate, weights, biases)

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) # Softmax loss
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer

# Evaluate model
correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Initializing the variables
init = tf.initialize_all_variables()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)
    step = 1
    # Keep training until reach max iterations
    while step * batch_size &lt; training_iters:
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        # Reshape data to get 28 seq of 28 elements
        batch_xs = batch_xs.reshape((batch_size, n_steps, n_input))
        # Fit training using batch data
        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys,
                                       istate: np.zeros((batch_size, 2*n_hidden))})
        if step % display_step == 0:
            # Calculate batch accuracy
            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys,
                                                istate: np.zeros((batch_size, 2*n_hidden))})
            # Calculate batch loss
            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys,
                                             istate: np.zeros((batch_size, 2*n_hidden))})
            print &quot;Iter &quot; + str(step*batch_size) + &quot;, Minibatch Loss= &quot; + &quot;{:.6f}&quot;.format(loss) + \
                  &quot;, Training Accuracy= &quot; + &quot;{:.5f}&quot;.format(acc)
        step += 1
    print &quot;Optimization Finished!&quot;
    # Calculate accuracy for 256 mnist test images
    test_len = 256
    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))
    test_label = mnist.test.labels[:test_len]
    print &quot;Testing Accuracy:&quot;, sess.run(accuracy, feed_dict={x: test_data, y: test_label,
                                                             istate: np.zeros((test_len, 2*n_hidden))})
</code></pre><h1 id="使用TensorFlow实现RNN"><a href="#使用TensorFlow实现RNN" class="headerlink" title="使用TensorFlow实现RNN"></a>使用TensorFlow实现RNN</h1><p><a href="http://blog.csdn.net/kkk584520/article/details/51477830" target="_blank" rel="external">RNN实现</a><br><a href="http://blog.csdn.net/u012436149/article/details/53026848" target="_blank" rel="external">tensorflow.Python.ops.rnn_cell</a></p>
<p>类型判断：<br>    isinstance<br>rnn_cell中的函数：<br>    def _linear(args, output_size, bias, bias_start=0.0, scope=None):<br>args: list of tensor [batch_size, size]. 注意,list中的每个tensor的size 并不需要一定相同,但batch_size要保证一样.<br>output_size : 一个整数<br>bias: bool型, True表示 加bias,False表示不加<br>return : [batch_size, output_size]<br>注意: 这个函数的atgs 不能是 _ref 类型(tf_getvariable(), tf.Variables()返回的都是 _ref),<br>但这个 _ref类型经过任何op之后,_ref就会消失</p>
<pre><code>class BasicLSTMCell(RNNCell):
    def __init__(self, num_units, forget_bias=1.0, input_size=None,
           state_is_tuple=True, activation=tanh):
&quot;&quot;&quot;
为什么被称为 Basic
It does not allow cell clipping, a projection layer, and does not
use peep-hole connections: it is the basic baseline.
&quot;&quot;&quot;
</code></pre><p>num_units: lstm单元的output_size<br>input_size: 这个参数没必要输入, 官方说马上也要禁用了<br>state_is_tuple: True的话, (c_state,h_state)作为tuple返回<br>activation: 激活函数<br>注意: 在我们创建 cell=BasicLSTMCell(…) 的时候, 只是初始化了cell的一些基本参数值. 这时,是没有variable被创建的, variable在我们 cell(input, state)时才会被创建, 下面所有的类都是这样</p>
<pre><code>class GRUCell(RNNCell):
    def __init__(self, num_units, input_size=None, activation=tanh):
</code></pre><p>创建一个GRUCell</p>
<pre><code>class OutputProjectionWrapper(RNNCell):
    def __init__(self, cell, output_size):
</code></pre><p>output_size: 要映射的 size<br>return: 返回一个 带有 OutputProjection Layer的 cell(s)</p>
<pre><code>class InputProjectionWrapper(RNNCell):
  def __init__(self, cell, num_proj, input_size=None):
</code></pre><p>和上面差不多,一个输出映射,一个输入映射</p>
<pre><code>class DropoutWrapper(RNNCell):
  def __init__(self, cell, input_keep_prob=1.0, output_keep_prob=1.0,
               seed=None):
</code></pre><p>dropout</p>
<pre><code>class EmbeddingWrapper(RNNCell):
  def __init__(self, cell, embedding_classes, embedding_size, initializer=None):
</code></pre><p>返回一个带有 embedding 的cell</p>
<pre><code>class MultiRNNCell(RNNCell):
  def __init__(self, cells, state_is_tuple=True):
</code></pre><p>用来增加 rnn 的层数<br>cells : list of cell<br>返回一个多层的cell</p>
<p><a href="http://blog.csdn.net/u010089444/article/details/60963053" target="_blank" rel="external">tensorflow1.0中的RNN</a></p>
<p><a href="http://blog.csdn.net/guotong1988/article/details/52767418" target="_blank" rel="external">tensorflow reverse_sequence实例</a></p>
<p>class tf.contrib.rnn.BasicLSTMCell</p>
<p><a href="http://blog.csdn.net/u013378306/article/details/62427462" target="_blank" rel="external">tensorflow常用函数</a><br><a href="http://blog.csdn.net/u010417185/article/details/51900441" target="_blank" rel="external">转置函数 transpose</a></p>
<h1 id="最强网络RSNN深度残差网络-平均准确率96-99"><a href="#最强网络RSNN深度残差网络-平均准确率96-99" class="headerlink" title="最强网络RSNN深度残差网络 平均准确率96-99%"></a>最强网络RSNN深度残差网络 平均准确率96-99%</h1><p>论文：图像识别的深度残差学习 Deep Residual Learning for Image Recognition<br>待完善</p>
<h1 id="最强网络DLSTM-双向长短期记忆网络（阿里小AI实现）"><a href="#最强网络DLSTM-双向长短期记忆网络（阿里小AI实现）" class="headerlink" title="最强网络DLSTM 双向长短期记忆网络（阿里小AI实现）"></a>最强网络DLSTM 双向长短期记忆网络（阿里小AI实现）</h1><p>并没有给出阿里小AI的完整实现<br>outputs = rnn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, _X,<br>                                            initial_state_fw=_istate_fw,<br>                                            initial_state_bw=_istate_bw,<br>                                            sequence_length=_seq_len)</p>
<pre><code># Linear activation
# Get inner loop last output
return tf.matmul(outputs[-1], _weights[&apos;out&apos;]) + _biases[&apos;out&apos;]
</code></pre><h1 id="Tensorflow-Caffe相互转换"><a href="#Tensorflow-Caffe相互转换" class="headerlink" title="Tensorflow Caffe相互转换"></a>Tensorflow Caffe相互转换</h1><pre><code>开源项目： https://github.com/ethereon/caffe-tensorflow 
./convert.py /root/googleNet.prototxt --code-output-path=googleNetTensorflow.py
</code></pre><h1 id="Tensorflow-RCNN区域卷积神经网络"><a href="#Tensorflow-RCNN区域卷积神经网络" class="headerlink" title="Tensorflow RCNN区域卷积神经网络"></a>Tensorflow RCNN区域卷积神经网络</h1><h1 id="模型AutoEncoder自编码机网络"><a href="#模型AutoEncoder自编码机网络" class="headerlink" title="模型AutoEncoder自编码机网络"></a>模型AutoEncoder自编码机网络</h1><pre><code>AutoEncoder 用于 有监督里面 可以用来 数据压缩 数据降维 过滤 降噪 
AutoEncoder用于 无监督算法 里面可以用来进行 数据的转换 数据的自学习
</code></pre><h1 id="Tensorflow人工智能分布式实现"><a href="#Tensorflow人工智能分布式实现" class="headerlink" title="Tensorflow人工智能分布式实现"></a>Tensorflow人工智能分布式实现</h1><ol>
<li>采用spark 的RDD 结合 map 在 map里面执行每一层 每一层 FF BP 采用 broardCast传播全局变量 也就是计算的梯度 来接着 map里面进行 更新梯度操作</li>
<li>spark 的mllib包</li>
<li>PipeLine管道 ，mlib机器学习的 ml管道方式</li>
<li>分布式数据库 分布式调度</li>
</ol>
<h1 id="Tensorflow实战"><a href="#Tensorflow实战" class="headerlink" title="Tensorflow实战"></a>Tensorflow实战</h1><p>《TensorFlow实战》这本书的主要内容：</p>
<ol>
<li>使用数据流式图规划计算流程</li>
<li>多个设备时：<br> 每一个节点该让什么硬件设备执行–设计了一套策略，首先需要计算代价模型。<br> 如何管理节点间的数据通信–子图之间的发送节点和接受节点 TCP或者RDMA</li>
<li>tf.device(“/gpu:%d”%d) 从一块GPU到多块的代码</li>
<li>支持自动求导[db,dW,dx]=tf.gradients(C,[b,W,x])</li>
<li>反向传播时可能需要初始Tensor,会占用内存。持续改进中</li>
<li>用户可以选择计算图的任意子图，节点名+port的形式</li>
<li>计算图的控制流：if-condition while-loop</li>
<li>接收节点在刚好需要数据的时候才开始接收数据</li>
<li>receive,enqueue,dequeue异步的实现</li>
<li>加速：数据并行，模型并行，流水线并行</li>
<li>Python等接口是通过SWIG实现的</li>
<li>兼容Scikit-learn estimator接口</li>
<li>计算图必须构建为静态图，beam search变得困难</li>
<li>不同设备间通信使用基于socket的RPC</li>
<li>Tensorflow Serving</li>
<li>tf.variables中的参数是持久化的</li>
<li>tf.cast(correct_prediction,tf.float32)# 类型转换</li>
<li>accuracy.eval(feedict)</li>
<li>xavier initialization 初始化参数</li>
<li>tf.truncated_normal([in_units,h1_units]) #截断的正态分布</li>
<li>tf.nn.lrn对局部神经元的活动创建竞争环境，使得其中响应比较大的值变得相对更大</li>
<li>tf.train.start_queue_runners()#启动线程进行加速</li>
<li>tf.random_uniform((fan_in,fan_out),minval=low,maxval=high,dtype=tf.float32)#均匀分布</li>
<li>tf.truncated_normal([in_units,h1_units],stddev=0.1)#截断的正态分布，标准差为0.1</li>
<li>tf.nn.in_top_k(logits,label_holder,1)#输出结果中topk的准确率</li>
<li>withtf.namescope(‘conv1’) as scope可以将scope内生成的Variable自动命名为conv1/name</li>
<li>tf.nn.nce_loss()Noise-contrastive Estimation</li>
<li><em>,a 中</em>只是一个占位的作用</li>
<li></li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

<blockquote class="blockquote-center" style="color: #ccc;">
    -------------本文结束 <i class="fa fa-apple"></i> 感谢您的阅读-------------
</blockquote>

  <span id="inline-green" style="border-radius:3px;">作者</span>：<a class="link-blue" href="https://github.com/GonewithGt" target="_blank">GonewithGt</a><br/>有问题请 <a class="link-blue" href="https://GonewithGt.github.io/guestbook" target="_blank">留言</a> 或者私信我的 <a class="link-blue" href="http://weibo.com/1134446353" target="_blank">微博</a>。

  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>满分是10分的话，这篇文章你给几分</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/reward/reward_wechat.png" alt="GonewithGt WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
    </div>
  </div>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DL/" rel="tag"><i class="fa fa-tag"></i> DL</a>
          
            <a href="/tags/TensorFlow/" rel="tag"><i class="fa fa-tag"></i> TensorFlow</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/03/17/智能算法/" rel="next" title="智能算法">
                <i class="fa fa-chevron-left"></i> 智能算法
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/03/23/并发/" rel="prev" title="并发">
                并发 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    <div id="gitmentContainer" style="margin-bottom: -19px;"></div>
    <link rel="stylesheet" href="/css/gitment.css">
    <script src="/js/src/gitment.browser.js" type="text/javascript"></script>
    <script type="text/javascript">
      var gitment = new Gitment({
        id: 'blog-guestbook',
        owner: 'GonewithGt',
        repo: 'GonewithGt.github.io',
        oauth: {
          client_id: '',
          client_secret: '',
        },
      })
      gitment.render('gitmentContainer')
    </script>

    
  </div>

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="GonewithGt" />
          <p class="site-author-name" itemprop="name">GonewithGt</p>
          <p class="site-description motion-element" itemprop="description">Everything is good now~</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">33</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">24</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/GonewithGt" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:1724532024@qq.com" target="_blank" title="Email">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Email
                </a>
              </span>
            
          
        </div>

        
        

        
        
      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Referrence"><span class="nav-number">1.</span> <span class="nav-text">Referrence</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">2.</span> <span class="nav-text">Reference</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tensorflow自学之前的bigpicture"><span class="nav-number">3.</span> <span class="nav-text">tensorflow自学之前的bigpicture</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#基本概念以及理解"><span class="nav-number">4.</span> <span class="nav-text">基本概念以及理解</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#安装和使用"><span class="nav-number">5.</span> <span class="nav-text">安装和使用</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CNN卷积神经网络的基本定义理解"><span class="nav-number">6.</span> <span class="nav-text">CNN卷积神经网络的基本定义理解</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实现一个自创的CNN卷积神经网络"><span class="nav-number">7.</span> <span class="nav-text">实现一个自创的CNN卷积神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorBoard面板可视化管理"><span class="nav-number">8.</span> <span class="nav-text">TensorBoard面板可视化管理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#tensorflow-网络可视化"><span class="nav-number">8.1.</span> <span class="nav-text">tensorflow 网络可视化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4D数据虚拟化"><span class="nav-number">8.2.</span> <span class="nav-text">4D数据虚拟化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图标变化方式展示tensorflow数据特征"><span class="nav-number">8.3.</span> <span class="nav-text">图标变化方式展示tensorflow数据特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#以变量批次事件变化的线路图显示表示tensorflow数据"><span class="nav-number">8.4.</span> <span class="nav-text">以变量批次事件变化的线路图显示表示tensorflow数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#汇总"><span class="nav-number">8.5.</span> <span class="nav-text">汇总</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AlphaGo-的策略网络（CNN）简单的实现"><span class="nav-number">9.</span> <span class="nav-text">AlphaGo 的策略网络（CNN）简单的实现</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#训练的模型Model保存文件并使用"><span class="nav-number">10.</span> <span class="nav-text">训练的模型Model保存文件并使用</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DNN深度神经网络的原理以及使用"><span class="nav-number">11.</span> <span class="nav-text">DNN深度神经网络的原理以及使用</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#接着补充一章MLP多层感知器网络原理以及使用"><span class="nav-number">12.</span> <span class="nav-text">接着补充一章MLP多层感知器网络原理以及使用</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RNN循环网络原理以及使用"><span class="nav-number">13.</span> <span class="nav-text">RNN循环网络原理以及使用</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#使用TensorFlow实现RNN"><span class="nav-number">14.</span> <span class="nav-text">使用TensorFlow实现RNN</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#最强网络RSNN深度残差网络-平均准确率96-99"><span class="nav-number">15.</span> <span class="nav-text">最强网络RSNN深度残差网络 平均准确率96-99%</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#最强网络DLSTM-双向长短期记忆网络（阿里小AI实现）"><span class="nav-number">16.</span> <span class="nav-text">最强网络DLSTM 双向长短期记忆网络（阿里小AI实现）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensorflow-Caffe相互转换"><span class="nav-number">17.</span> <span class="nav-text">Tensorflow Caffe相互转换</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensorflow-RCNN区域卷积神经网络"><span class="nav-number">18.</span> <span class="nav-text">Tensorflow RCNN区域卷积神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#模型AutoEncoder自编码机网络"><span class="nav-number">19.</span> <span class="nav-text">模型AutoEncoder自编码机网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensorflow人工智能分布式实现"><span class="nav-number">20.</span> <span class="nav-text">Tensorflow人工智能分布式实现</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensorflow实战"><span class="nav-number">21.</span> <span class="nav-text">Tensorflow实战</span></a></li></ol></div>
            
          </div>
        </section>
      
      <!-- hack一个虚线出来 -->
      <section style="border-top:1px dotted #ccc;height:10px;"></section>
      <!-- weibo show -->
      <iframe width="100%" height="120" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=120&fansRow=2&ptype=1&speed=0&skin=5&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=1134446353&verifier=d529ff3a&dpc=1"></iframe>
    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        

        <div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">GonewithGt</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io" rel="external nofollow">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next" rel="external nofollow">
    NexT.Pisces
  </a>
</div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count" style="color: #e90f92;">全站共  字</span>
</div>
        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = true;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '' && data_content != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title < 0 && index_content < 0 ){
                                isMatch = false;
                            } else {
                                if (index_content < 0) {
                                    index_content = 0;
                                }
                                if (i == 0) {
                                    first_occur = index_content;
                                }
                            }
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });

                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

  <!-- 按需加载背景 -->
  <!-- 按需加载背景 -->
<!-- 识别手机或电脑的js开始 -->  
<script type="text/javascript">   
(function(){  
var res = GetRequest();  
var par = res['index'];  
if(par!='gfan'){  
  var ua=navigator.userAgent.toLowerCase();  
  var contains=function (a, b){  
      if(a.indexOf(b)!=-1){return true;}  
  };   
  if((contains(ua,"android") && contains(ua,"mobile"))||(contains(ua,"android") && contains(ua,"mozilla"))||(contains(ua,"android") && contains(ua,"opera"))||contains(ua,"ucweb7")||contains(ua,"iphone")){
    return false;
  } else {
    $.getScript("/js/src/particle.js?v=5.0.1");
  }
}  
})();  
function GetRequest() {  
var url = location.search;
var theRequest = new Object();  
if (url.indexOf("?") != -1) {  
  var str = url.substr(1);  
  strs = str.split("&");  
  for(var i = 0; i < strs.length; i ++) {  
    theRequest[strs[i].split("=")[0]]=unescape(strs[i].split("=")[1]);  
  }  
}  
return theRequest;  
}  
</script>  
<!-- 识别手机或电脑的js结束 -->  
  <!-- 页面点击小红心 -->
  <!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js?v=5.0.1"></script>
  <!-- 页面移动stars -->
  <!-- 鼠标移动特效 -->
<script type="text/javascript" src="/js/src/jquery-stars.js?v=5.0.1"></script>
<script type="text/javascript">
jQuery('body').jstars({
	image_path: '/images',
	image: 'candy-cane-stars.png',
	style: 'white',
	width: 34,
	height: 34,
	delay: 700,
	frequency: 5
});
</script>
  <!-- 页面 title 进入/离开 效果 -->
<script type="text/javascript">var OriginTitile=document.title,st;document.addEventListener("visibilitychange",function(){document.hidden?(document.title="≡[。。]≡ 回不去了吗？!",clearTimeout(st)):(document.title="(ฅ>ω<*ฅ) Thank Vue~ "+OriginTitile,st=setTimeout(function(){document.title=OriginTitile},4e3))})</script>
</body>
</html>
